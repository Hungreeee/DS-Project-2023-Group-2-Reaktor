{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.special import comb\n",
    "import scipy.misc\n",
    "scipy.misc.comb = comb\n",
    "from ecopy import diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API endpoint\n",
    "URL = 'https://api.laji.fi/v0'\n",
    "# Your email\n",
    "EMAIL = 'behramulukir@aalto.fi'\n",
    "# Token ID\n",
    "TOKEN = 'GDHiX5rLpPzfjGK2owsbJLEkWCwQoLUnmtIGPUEqh40fGxmNSpyYf56jB3WTEDaQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error': {'statusCode': 422,\n",
       "  'name': 'ValidationError',\n",
       "  'message': 'The `APIUser` instance is not valid. Details: `email` Email already exists (value: \"behramulukir@aalto.fi\").',\n",
       "  'details': {'context': 'APIUser',\n",
       "   'codes': {'email': ['uniqueness']},\n",
       "   'messages': {'email': ['Email already exists']}}}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How to request a token\n",
    "# Just query a POST with your email. You will get the token in an email they sent you.\n",
    "response = requests.post(\n",
    "  url = URL + '/api-users',\n",
    "  data = {\n",
    "    'email': EMAIL\n",
    "  })\n",
    "\n",
    "# Check if it has been registered to the site\n",
    "responseContent = json.loads(response.content.decode('utf-8'))\n",
    "responseContent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial request to get the last page\n",
    "response = requests.get(\n",
    "  url = URL + '/warehouse/query/unit/list',\n",
    "  params={\n",
    "    'access_token': TOKEN,\n",
    "    'page': 1,\n",
    "    'pageSize': 10000, # Limit 10,000 per page\n",
    "    'taxonRankId': '93671, 83190, 65513, 66530, 82625, 99327', # {Vascular plants: {Plant life forms: {Trees; Evergreen trees} } }\n",
    "    'orderBy': ['gathering.eventDate.begin DESC', 'document.loadDate DESC', 'unit.taxonVerbatim ASC'] # Default sorting\n",
    "  })\n",
    "\n",
    "responseContent = response.content\n",
    "results = json.loads(responseContent.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lastPage = results['lastPage']\n",
    "\n",
    "# Lists of variables\n",
    "date = []\n",
    "latitude = []\n",
    "longitude = []\n",
    "municipality = []\n",
    "taxonId = []\n",
    "science = []\n",
    "verbatim = []\n",
    "\n",
    "for pageIndex in range(1, lastPage + 1):\n",
    "    # Request data from API and go through all the pages in the loop (approximately 8m with 12 pages and 115739 rows)\n",
    "    response = requests.get(\n",
    "        url = URL + '/warehouse/query/unit/list',\n",
    "        params={\n",
    "            'access_token': TOKEN,\n",
    "            'page': pageIndex,\n",
    "            'pageSize': 10000, # Limit 10,000 per page\n",
    "            'taxonRankId': '93671, 83190, 65513, 66530, 82625, 99327', # {Vascular plants: {Plant life forms: {Trees; Evergreen trees} } }\n",
    "            'orderBy': ['gathering.eventDate.begin DESC', 'document.loadDate DESC', 'unit.taxonVerbatim ASC'] # Default sorting\n",
    "    })\n",
    "\n",
    "    responseContent = response.content\n",
    "    dataset = json.loads(responseContent.decode('utf-8'))['results']\n",
    "\n",
    "    # Extract relevant information, sometimes they might be missing\n",
    "    for x in dataset:\n",
    "        # Date of collection; normally it would be \"DD-MM-YYYY\", sometimes it could be a range \"DD-MM-YYY - DD-MM-YYYY\"\n",
    "        if 'displayDateTime' in x['gathering']:\n",
    "            date.append(x['gathering']['displayDateTime'])\n",
    "        else:\n",
    "            date.append(None)\n",
    "\n",
    "        # Latitude and Longitude\n",
    "        if 'conversions' in x['gathering']:\n",
    "            latitude.append(x['gathering']['conversions']['wgs84CenterPoint']['lat'])\n",
    "            longitude.append(x['gathering']['conversions']['wgs84CenterPoint']['lon'])\n",
    "        else:\n",
    "            latitude.append(None)\n",
    "            longitude.append(None)\n",
    "\n",
    "        # Municipality\n",
    "        if 'interpretations' in x['gathering']  and 'municipalityDisplayname' in x['gathering']['interpretations']:\n",
    "            municipality.append(x['gathering']['interpretations']['municipalityDisplayname'])\n",
    "        else:\n",
    "            municipality.append(None)\n",
    "\n",
    "        # Taxon ID and Scientific Name\n",
    "        if 'unit' in x and 'linkings' in x['unit']:\n",
    "            taxonId.append(x['unit']['linkings']['taxon']['id'])\n",
    "            science.append(x['unit']['linkings']['taxon']['scientificName'])\n",
    "        else:\n",
    "            taxonId.append(None)\n",
    "            science.append(None)\n",
    "\n",
    "        # Taxon Verbatim\n",
    "        if 'unit' in x and 'taxonVerbatim' in x['unit']:\n",
    "            verbatim.append(x['unit']['taxonVerbatim'])\n",
    "        else:\n",
    "            verbatim.append(None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Date, Latitude, Longitude, Municipality, TaxonID, ScientificName, verbatim]\n",
      "Index: []\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: '../../correlation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/username-u/DS-Project-2023-Group-2-Reaktor/project-files/correlation/loadcorrelationdata.ipynb Cell 6\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell://github/username-u/DS-Project-2023-Group-2-Reaktor/project-files/correlation/loadcorrelationdata.ipynb#W5sdnNjb2RlLXZmcw%3D%3D?line=5'>6</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(data\u001b[39m=\u001b[39mdata)\n\u001b[1;32m      <a href='vscode-notebook-cell://github/username-u/DS-Project-2023-Group-2-Reaktor/project-files/correlation/loadcorrelationdata.ipynb#W5sdnNjb2RlLXZmcw%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(df)\n\u001b[0;32m----> <a href='vscode-notebook-cell://github/username-u/DS-Project-2023-Group-2-Reaktor/project-files/correlation/loadcorrelationdata.ipynb#W5sdnNjb2RlLXZmcw%3D%3D?line=7'>8</a>\u001b[0m df\u001b[39m.\u001b[39;49mto_csv(\u001b[39m\"\u001b[39;49m\u001b[39m../../correlation/correlationdata.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m, index\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/generic.py:3902\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3891\u001b[0m df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m, ABCDataFrame) \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_frame()\n\u001b[1;32m   3893\u001b[0m formatter \u001b[39m=\u001b[39m DataFrameFormatter(\n\u001b[1;32m   3894\u001b[0m     frame\u001b[39m=\u001b[39mdf,\n\u001b[1;32m   3895\u001b[0m     header\u001b[39m=\u001b[39mheader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3899\u001b[0m     decimal\u001b[39m=\u001b[39mdecimal,\n\u001b[1;32m   3900\u001b[0m )\n\u001b[0;32m-> 3902\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[39m.\u001b[39;49mto_csv(\n\u001b[1;32m   3903\u001b[0m     path_or_buf,\n\u001b[1;32m   3904\u001b[0m     lineterminator\u001b[39m=\u001b[39;49mlineterminator,\n\u001b[1;32m   3905\u001b[0m     sep\u001b[39m=\u001b[39;49msep,\n\u001b[1;32m   3906\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m   3907\u001b[0m     errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m   3908\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m   3909\u001b[0m     quoting\u001b[39m=\u001b[39;49mquoting,\n\u001b[1;32m   3910\u001b[0m     columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[1;32m   3911\u001b[0m     index_label\u001b[39m=\u001b[39;49mindex_label,\n\u001b[1;32m   3912\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[1;32m   3913\u001b[0m     chunksize\u001b[39m=\u001b[39;49mchunksize,\n\u001b[1;32m   3914\u001b[0m     quotechar\u001b[39m=\u001b[39;49mquotechar,\n\u001b[1;32m   3915\u001b[0m     date_format\u001b[39m=\u001b[39;49mdate_format,\n\u001b[1;32m   3916\u001b[0m     doublequote\u001b[39m=\u001b[39;49mdoublequote,\n\u001b[1;32m   3917\u001b[0m     escapechar\u001b[39m=\u001b[39;49mescapechar,\n\u001b[1;32m   3918\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m   3919\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/formats/format.py:1152\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1131\u001b[0m     created_buffer \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m csv_formatter \u001b[39m=\u001b[39m CSVFormatter(\n\u001b[1;32m   1134\u001b[0m     path_or_buf\u001b[39m=\u001b[39mpath_or_buf,\n\u001b[1;32m   1135\u001b[0m     lineterminator\u001b[39m=\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     formatter\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfmt,\n\u001b[1;32m   1151\u001b[0m )\n\u001b[0;32m-> 1152\u001b[0m csv_formatter\u001b[39m.\u001b[39;49msave()\n\u001b[1;32m   1154\u001b[0m \u001b[39mif\u001b[39;00m created_buffer:\n\u001b[1;32m   1155\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/formats/csvs.py:247\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[39mCreate the writer & save.\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[39m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[0;32m--> 247\u001b[0m \u001b[39mwith\u001b[39;00m get_handle(\n\u001b[1;32m    248\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilepath_or_buffer,\n\u001b[1;32m    249\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    250\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    251\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merrors,\n\u001b[1;32m    252\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompression,\n\u001b[1;32m    253\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstorage_options,\n\u001b[1;32m    254\u001b[0m ) \u001b[39mas\u001b[39;00m handles:\n\u001b[1;32m    255\u001b[0m     \u001b[39m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[1;32m    256\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter \u001b[39m=\u001b[39m csvlib\u001b[39m.\u001b[39mwriter(\n\u001b[1;32m    257\u001b[0m         handles\u001b[39m.\u001b[39mhandle,\n\u001b[1;32m    258\u001b[0m         lineterminator\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m         quotechar\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquotechar,\n\u001b[1;32m    264\u001b[0m     )\n\u001b[1;32m    266\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/common.py:739\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    737\u001b[0m \u001b[39m# Only for write methods\u001b[39;00m\n\u001b[1;32m    738\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode \u001b[39mand\u001b[39;00m is_path:\n\u001b[0;32m--> 739\u001b[0m     check_parent_directory(\u001b[39mstr\u001b[39;49m(handle))\n\u001b[1;32m    741\u001b[0m \u001b[39mif\u001b[39;00m compression:\n\u001b[1;32m    742\u001b[0m     \u001b[39mif\u001b[39;00m compression \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mzstd\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    743\u001b[0m         \u001b[39m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/common.py:604\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    602\u001b[0m parent \u001b[39m=\u001b[39m Path(path)\u001b[39m.\u001b[39mparent\n\u001b[1;32m    603\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m parent\u001b[39m.\u001b[39mis_dir():\n\u001b[0;32m--> 604\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\u001b[39mrf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCannot save file into a non-existent directory: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mparent\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mOSError\u001b[0m: Cannot save file into a non-existent directory: '../../correlation'"
     ]
    }
   ],
   "source": [
    "# Merge data into a dataframe; there are a total of 115739 observations, but only 115722 rows in the dataframe. This could be due to my variables selection, e.g. there are\n",
    "# some observations that have none of the listed variables (no name, no time, no place)\n",
    "\n",
    "data = {\"Date\": date, \"Latitude\": latitude, \"Longitude\": longitude, \"Municipality\": municipality, \"TaxonID\": taxonId, \"ScientificName\": science,\n",
    "        \"verbatim\": verbatim}\n",
    "df = pd.DataFrame(data=data)\n",
    "print(df)\n",
    "df.to_csv(\"../../correlation/correlationdata.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
